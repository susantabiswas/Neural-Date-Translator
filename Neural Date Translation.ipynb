{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Neural Date Translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date translation from Human readable format to machine readable format(YYYY-MM-DD) using Recurrent Neural Network.**\n",
    "For example a date in human readable format can be : 'sunday 15 september 2013', '29-oct-1997' or '30 august 1985' etc.\n",
    "<br>The task is to convert this to a more normalized format that is **YYYY-MM-DD**.\n",
    "\n",
    "For this task a **sequence to sequence encoder-decoder** network with **LSTM units** have been used along with **Attention Mechanism**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector, Dense, Activation\n",
    "from keras.layers import Bidirectional, Concatenate, Dot, LSTM, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "\n",
    "import os.path\n",
    "import numpy as np\n",
    "import random\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Loading\n",
    "We will train the neural network on a dataset of 10000 human readable dates(e.g. \"the 19th of July 2016\", \"23/02/2017\") and their equivalent machine readable dates that will be in format **DD-MM-YYYY**.\n",
    "\n",
    "For generating the training examples we will be using Faker.\n",
    "Code for generating the training examples and for preprocessing is there in the utility file.\n",
    "1. First we generate the training examples\n",
    "2. We make mappings from characters used in the dates to numerical indices and vice-versa.\n",
    "3. We make one hot encodings for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from faker import Faker\n",
    "from babel.dates import format_date\n",
    "\n",
    "# for creating a single fake date\n",
    "def create_date(fake_obj, DATE_FORMATS):\n",
    "    \n",
    "    try:\n",
    "        # create a date object\n",
    "        dt = fake_obj.date_object()\n",
    "        # create machine readable dates\n",
    "        machine_read_dates = dt.isoformat()\n",
    "        # get human readable dates\n",
    "        human_read_dates = format_date(dt, format=random.choice(DATE_FORMATS),\n",
    "                            locale='en_IN')\n",
    "        # remove punctuations\n",
    "        human_read_dates = human_read_dates.replace(',', '')\n",
    "        # change to lower case\n",
    "        human_read_dates = human_read_dates.lower()\n",
    "    \n",
    "    except AttributeError as e:\n",
    "        return None, None\n",
    "    \n",
    "    return human_read_dates, machine_read_dates\n",
    "\n",
    "\n",
    "# for creating 'm' training dataset\n",
    "def create_dataset(m):\n",
    "    '''\n",
    "    Arg:\n",
    "        m: no. of training examples\n",
    "    Returns:\n",
    "    dataset --list: for saving list of tuples of date pairs\n",
    "    human_vocab --set: for saving human readable dates vocabulary\n",
    "    machine_vocab -- set: for saving machine readable dates vocabulary\n",
    "    \n",
    "    '''\n",
    "    # for generating fake training data\n",
    "    fake_obj = Faker()\n",
    "    \n",
    "    # date formats for generating date\n",
    "    # one of them is selected randomly each time that is why full is mentioned\n",
    "    # many times to increase its chances\n",
    "    DATE_FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "    # for saving the dataset\n",
    "    dataset = []\n",
    "    # for saving human readable dates vocabulary\n",
    "    human_vocab = set()\n",
    "    # for saving machine readable dates vocabulary\n",
    "    machine_vocab = set()\n",
    "\n",
    "    for i in range(m):\n",
    "        human_date, machine_date = create_date(fake_obj, DATE_FORMATS)\n",
    "        if human_date is not None:\n",
    "            # add date to dataset\n",
    "            dataset.append((human_date, machine_date))\n",
    "            # add new vocabulary entry\n",
    "            for char in human_date:\n",
    "                if char not in human_vocab:\n",
    "                    human_vocab.add(char)\n",
    "            for char in machine_date:\n",
    "                if char not in machine_vocab:\n",
    "                    machine_vocab.add(char)\n",
    "            \n",
    "    human_vocab = sorted(human_vocab)\n",
    "    machine_vocab = sorted(machine_vocab)\n",
    "    \n",
    "    return dataset, human_vocab, machine_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(m, dataset, human_char_idx, machine_char_idx, Tx, Ty):\n",
    "    # separate the tuples\n",
    "    X, Y = zip(*dataset)\n",
    "    \n",
    "    # make numpy arrays to store the X and Y data\n",
    "    X_ohe = np.zeros((m, Tx, len(human_char_idx)), dtype = 'float32')\n",
    "    Y_ohe = np.zeros((m, Ty, len(machine_char_idx)), dtype = 'float32')\n",
    "    \n",
    "    \n",
    "    # truncate the length of date if it exceeds Tx\n",
    "    for i, date in enumerate(X):\n",
    "        if len(date) > Tx:\n",
    "            X[i] = X[:Tx]\n",
    "            \n",
    "    # now do one hot encoding\n",
    "    for i in range(m):\n",
    "        for timestep, char in enumerate(X[i]):\n",
    "            X_ohe[i, timestep, human_char_idx[char]] = 1\n",
    "        for timestep, char in enumerate(Y[i]):\n",
    "            Y_ohe[i, timestep, machine_char_idx[char]] = 1\n",
    "            \n",
    "    return X, Y, X_ohe, Y_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(m, Tx, Ty):\n",
    "    dataset, human_vocab, machine_vocab = create_dataset(m)\n",
    "    # add the unknown and pad characters\n",
    "    #human_vocab += ['<UNK>', '<PAD>']\n",
    "    \n",
    "    # now we will create a dictionary for mapping the vocabulary tokens to numerical indices\n",
    "    human_char_idx = dict((token, i) for i, token in enumerate(human_vocab) )\n",
    "    # reverse mapping from indices to tokens for machine readable dates\n",
    "    machine_idx_char = dict(enumerate(machine_vocab))\n",
    "    # mapping from char to indices for machine readable dates\n",
    "    machine_char_idx = dict((token, i) for i, token in enumerate(machine_vocab) )\n",
    "\n",
    "    X, Y, X_ohe, Y_ohe = preprocess_data(m, dataset, human_char_idx, machine_char_idx, Tx, Ty)\n",
    "\n",
    "    return dataset, X, Y, X_ohe, Y_ohe, human_vocab, human_char_idx, machine_vocab, machine_char_idx, machine_idx_char    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of training examples\n",
    "m = 10000\n",
    "Tx = 30 # time steps for input\n",
    "Ty = 10 # time steps for output\n",
    "\n",
    "dataset, X, Y, X_ohe, Y_ohe, human_vocab, human_char_idx, machine_vocab, machine_char_idx, machine_idx_char = create_training_data(m, Tx, Ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monday 8 september 1997', '1997-09-08'),\n",
       " ('november 8 2017', '2017-11-08'),\n",
       " ('10 february 2010', '2010-02-10'),\n",
       " ('monday 5 july 1982', '1982-07-05'),\n",
       " ('30.06.02', '2002-06-30'),\n",
       " ('wednesday 6 august 1975', '1975-08-06'),\n",
       " ('16 december 2008', '2008-12-16'),\n",
       " ('saturday 16 january 1993', '1993-01-16'),\n",
       " ('wednesday 15 april 1981', '1981-04-15'),\n",
       " ('30 dec 2012', '2012-12-30')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30, 36)\n",
      "(10000, 10, 11)\n",
      "{' ': 0, '-': 1, '.': 2, '/': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13, 'a': 14, 'b': 15, 'c': 16, 'd': 17, 'e': 18, 'f': 19, 'g': 20, 'h': 21, 'i': 22, 'j': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'y': 35}\n",
      "\n",
      "{'-': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10}\n"
     ]
    }
   ],
   "source": [
    "print(X_ohe.shape)\n",
    "print(Y_ohe.shape)\n",
    "print( human_char_idx)\n",
    "print()\n",
    "print( machine_char_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "We will use an **encoder-decoder** network for for this task.<br>\n",
    "        Attention mechanism has also been incorporated to make the decoding better. The network uses **LSTM** cells\n",
    "in both decoder and encoder networks. The pre attention encoder network is actually a **bidirectional LSTM** and the post attention decoder is an **unidirectional LSTM** network.  <br>    \n",
    "        Post attention LSTM cells get the input from the context calculated from the attention weights. The output from each cell is not passed onto the next since it doesn't matter much what the previous chracter is in **YYYY-MM-DD** sequence.\n",
    "        \n",
    "To make sure that the layers share the same weight values throughout the different timesteps we will make layers defined as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared layers\n",
    "# Following layers are mainly for the neural network for finding the attention weights\n",
    "concatenator = Concatenate(axis = -1)\n",
    "repeator = RepeatVector(Tx)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation('softmax')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared layers\n",
    "# below layers are for the post LSTM network\n",
    "n_acti_pre = 32 # hidden activation units in pre attention LSTM network\n",
    "n_acti_post = 64 # hidden activation units in post attention LSTM network\n",
    "\n",
    "# post activation LSTM cell\n",
    "post_acti_LSTM = LSTM(n_acti_post, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation = 'softmax')\n",
    "\n",
    "# initial input states for post LSTM network\n",
    "ini_acti_post = np.zeros((m, n_acti_post))\n",
    "ini_mem_post = np.zeros((m, n_acti_post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting the context value using attention weights for each timestep of output\n",
    "def get_context(acti_pre, acti_post_prev):\n",
    "    \"\"\"\n",
    "    Finds the context value using attention weights for each timestep of output\n",
    "    Arguments:\n",
    "        acti_pre -- numpy-array(m, Tx, 2*n_acti_pre): hidden state values of  Bidirectional-LSTM network \n",
    "        acti_post_prev -- numpy array(m, n_acti_post): previous hidden state of the (post-attention) LSTM\n",
    "\n",
    "    Returns:\n",
    "        context -- vector, input of the next post-attetion LSTM cell\n",
    "    \"\"\"\n",
    "    # repeat the previous state of the post -attention LSTM cell\n",
    "    acti_post_prev = repeator(acti_post_prev)\n",
    "    # concatenate the output with the activations from the different pre attetntion LSTM cells\n",
    "    concat_vals = concatenator([acti_post_prev, acti_pre])\n",
    "    # pass the concatenated values through a dense layer\n",
    "    inter_vals = densor1(concat_vals)\n",
    "    inter_vals = densor2(inter_vals)\n",
    "    # pass the intermediate value through a softmax layer to get the alpha values\n",
    "    alpha_vals = activator(inter_vals)\n",
    "    # after getting the alpha values find the sum of weighted product of pre- activations values with their context weights(alpha values)\n",
    "    context = dotor([alpha_vals, acti_pre])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating a Keras model Instance\n",
    "def create_model(human_vocab_len, machine_vocab_len, Tx, Ty, n_acti_pre, n_acti_post):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        human_vocab_len - length of the human_char_idx dictionary\n",
    "        machine_vocab_len - length of the machine_char_idx dictionary\n",
    "        Tx - length of the input sequence\n",
    "        Ty - length of the output sequence\n",
    "        n_acti_pre - no. of hidden state units of the Bi-LSTM\n",
    "        n_acti_post - no. of hidden state units of the post-attention LSTM\n",
    "\n",
    "    Returns:\n",
    "        model -- Keras model instance\n",
    "    \"\"\"\n",
    "    # for storing the outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # define input for the model\n",
    "    X = Input(shape=(Tx, human_vocab_len))\n",
    "    # for the decoder LSTM i.e post attention network\n",
    "    # initial values\n",
    "    ini_acti_post = Input(shape=(n_acti_post,), name='ini_acti_post')\n",
    "    ini_mem_post = Input(shape=(n_acti_post,), name='ini_mem_post')\n",
    "   \n",
    "    \n",
    "    # current timestep values\n",
    "    mem_post = ini_mem_post\n",
    "    acti_post = ini_acti_post\n",
    "    \n",
    "    # make the encoder Bidirectional LSTM network\n",
    "    acti_pre = Bidirectional(LSTM(n_acti_pre, return_sequences=True))(X)\n",
    "    \n",
    "    # loop over each output timestep\n",
    "    for timestep in range(Ty):\n",
    "        # get the current context value\n",
    "        context = get_context(acti_pre, acti_post)\n",
    "        # feed the context value to the decoder LSTM network post attention\n",
    "        acti_post, _, mem_post = post_acti_LSTM (context, initial_state=[acti_post, mem_post])\n",
    "        # get the softmax output \n",
    "        target_output = output_layer(acti_post)\n",
    "        \n",
    "        # add the output target value\n",
    "        outputs.append(target_output)\n",
    "   \n",
    "    # make the model and return model instance\n",
    "    model = Model(inputs=[X, ini_acti_post, ini_mem_post], output=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUSANTA\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "model = create_model(len(human_vocab), len(machine_vocab), Tx, Ty, n_acti_pre, n_acti_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ini_acti_post (InputLayer)      (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 30, 36)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           ini_acti_post[0][0]              \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17664       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 30, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           activation_1[0][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[1][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[2][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[3][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[4][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[5][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[6][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[7][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[8][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 activation_1[9][0]               \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ini_mem_post (InputLayer)       (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 ini_acti_post[0][0]              \n",
      "                                                                 ini_mem_post[0][0]               \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,704\n",
      "Trainable params: 52,704\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights from any previously saved model\n",
    "model_path = r'models/weights_55.h5'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer and compile the model\n",
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 22s - loss: 0.4388 - dense_4_loss_1: 0.0135 - dense_4_loss_2: 0.0116 - dense_4_loss_3: 0.0811 - dense_4_loss_4: 0.0902 - dense_4_loss_5: 0.0014 - dense_4_loss_6: 0.0205 - dense_4_loss_7: 0.1163 - dense_4_loss_8: 0.0020 - dense_4_loss_9: 0.0344 - dense_4_loss_10: 0.0680 - dense_4_acc_1: 0.9954 - dense_4_acc_2: 0.9955 - dense_4_acc_3: 0.9770 - dense_4_acc_4: 0.9814 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9936 - dense_4_acc_7: 0.9707 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9978 - dense_4_acc_10: 0.9897\n",
      "Epoch 2/20\n",
      " - 22s - loss: 0.4207 - dense_4_loss_1: 0.0130 - dense_4_loss_2: 0.0109 - dense_4_loss_3: 0.0769 - dense_4_loss_4: 0.0858 - dense_4_loss_5: 0.0014 - dense_4_loss_6: 0.0199 - dense_4_loss_7: 0.1132 - dense_4_loss_8: 0.0020 - dense_4_loss_9: 0.0322 - dense_4_loss_10: 0.0654 - dense_4_acc_1: 0.9954 - dense_4_acc_2: 0.9953 - dense_4_acc_3: 0.9788 - dense_4_acc_4: 0.9840 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9933 - dense_4_acc_7: 0.9717 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9982 - dense_4_acc_10: 0.9914\n",
      "Epoch 3/20\n",
      " - 22s - loss: 0.4055 - dense_4_loss_1: 0.0122 - dense_4_loss_2: 0.0103 - dense_4_loss_3: 0.0734 - dense_4_loss_4: 0.0814 - dense_4_loss_5: 0.0014 - dense_4_loss_6: 0.0199 - dense_4_loss_7: 0.1117 - dense_4_loss_8: 0.0019 - dense_4_loss_9: 0.0308 - dense_4_loss_10: 0.0626 - dense_4_acc_1: 0.9958 - dense_4_acc_2: 0.9960 - dense_4_acc_3: 0.9799 - dense_4_acc_4: 0.9857 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9932 - dense_4_acc_7: 0.9718 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9981 - dense_4_acc_10: 0.9913\n",
      "Epoch 4/20\n",
      " - 22s - loss: 0.3908 - dense_4_loss_1: 0.0119 - dense_4_loss_2: 0.0098 - dense_4_loss_3: 0.0697 - dense_4_loss_4: 0.0774 - dense_4_loss_5: 0.0013 - dense_4_loss_6: 0.0186 - dense_4_loss_7: 0.1093 - dense_4_loss_8: 0.0018 - dense_4_loss_9: 0.0301 - dense_4_loss_10: 0.0608 - dense_4_acc_1: 0.9964 - dense_4_acc_2: 0.9962 - dense_4_acc_3: 0.9825 - dense_4_acc_4: 0.9879 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9944 - dense_4_acc_7: 0.9724 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9985 - dense_4_acc_10: 0.9915\n",
      "Epoch 5/20\n",
      " - 22s - loss: 0.3772 - dense_4_loss_1: 0.0114 - dense_4_loss_2: 0.0095 - dense_4_loss_3: 0.0670 - dense_4_loss_4: 0.0730 - dense_4_loss_5: 0.0013 - dense_4_loss_6: 0.0185 - dense_4_loss_7: 0.1063 - dense_4_loss_8: 0.0018 - dense_4_loss_9: 0.0288 - dense_4_loss_10: 0.0596 - dense_4_acc_1: 0.9961 - dense_4_acc_2: 0.9960 - dense_4_acc_3: 0.9826 - dense_4_acc_4: 0.9894 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9943 - dense_4_acc_7: 0.9730 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9983 - dense_4_acc_10: 0.9919\n",
      "Epoch 6/20\n",
      " - 22s - loss: 0.3619 - dense_4_loss_1: 0.0110 - dense_4_loss_2: 0.0090 - dense_4_loss_3: 0.0643 - dense_4_loss_4: 0.0698 - dense_4_loss_5: 0.0013 - dense_4_loss_6: 0.0176 - dense_4_loss_7: 0.1040 - dense_4_loss_8: 0.0017 - dense_4_loss_9: 0.0269 - dense_4_loss_10: 0.0563 - dense_4_acc_1: 0.9969 - dense_4_acc_2: 0.9965 - dense_4_acc_3: 0.9850 - dense_4_acc_4: 0.9908 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9944 - dense_4_acc_7: 0.9751 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9989 - dense_4_acc_10: 0.9926\n",
      "Epoch 7/20\n",
      " - 22s - loss: 0.3487 - dense_4_loss_1: 0.0106 - dense_4_loss_2: 0.0087 - dense_4_loss_3: 0.0612 - dense_4_loss_4: 0.0664 - dense_4_loss_5: 0.0012 - dense_4_loss_6: 0.0174 - dense_4_loss_7: 0.1011 - dense_4_loss_8: 0.0017 - dense_4_loss_9: 0.0259 - dense_4_loss_10: 0.0545 - dense_4_acc_1: 0.9967 - dense_4_acc_2: 0.9965 - dense_4_acc_3: 0.9851 - dense_4_acc_4: 0.9916 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9945 - dense_4_acc_7: 0.9759 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9989 - dense_4_acc_10: 0.9930\n",
      "Epoch 8/20\n",
      " - 21s - loss: 0.3376 - dense_4_loss_1: 0.0102 - dense_4_loss_2: 0.0084 - dense_4_loss_3: 0.0584 - dense_4_loss_4: 0.0635 - dense_4_loss_5: 0.0012 - dense_4_loss_6: 0.0172 - dense_4_loss_7: 0.0996 - dense_4_loss_8: 0.0016 - dense_4_loss_9: 0.0250 - dense_4_loss_10: 0.0524 - dense_4_acc_1: 0.9969 - dense_4_acc_2: 0.9966 - dense_4_acc_3: 0.9863 - dense_4_acc_4: 0.9918 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9949 - dense_4_acc_7: 0.9758 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9987 - dense_4_acc_10: 0.9938\n",
      "Epoch 9/20\n",
      " - 22s - loss: 0.3269 - dense_4_loss_1: 0.0100 - dense_4_loss_2: 0.0080 - dense_4_loss_3: 0.0564 - dense_4_loss_4: 0.0606 - dense_4_loss_5: 0.0012 - dense_4_loss_6: 0.0164 - dense_4_loss_7: 0.0974 - dense_4_loss_8: 0.0016 - dense_4_loss_9: 0.0242 - dense_4_loss_10: 0.0509 - dense_4_acc_1: 0.9967 - dense_4_acc_2: 0.9965 - dense_4_acc_3: 0.9872 - dense_4_acc_4: 0.9932 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9953 - dense_4_acc_7: 0.9770 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9987 - dense_4_acc_10: 0.9941\n",
      "Epoch 10/20\n",
      " - 22s - loss: 0.3166 - dense_4_loss_1: 0.0095 - dense_4_loss_2: 0.0076 - dense_4_loss_3: 0.0540 - dense_4_loss_4: 0.0575 - dense_4_loss_5: 0.0011 - dense_4_loss_6: 0.0166 - dense_4_loss_7: 0.0958 - dense_4_loss_8: 0.0015 - dense_4_loss_9: 0.0232 - dense_4_loss_10: 0.0497 - dense_4_acc_1: 0.9972 - dense_4_acc_2: 0.9970 - dense_4_acc_3: 0.9883 - dense_4_acc_4: 0.9939 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9956 - dense_4_acc_7: 0.9782 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9989 - dense_4_acc_10: 0.9943\n",
      "Epoch 11/20\n",
      " - 22s - loss: 0.3062 - dense_4_loss_1: 0.0092 - dense_4_loss_2: 0.0072 - dense_4_loss_3: 0.0513 - dense_4_loss_4: 0.0546 - dense_4_loss_5: 0.0011 - dense_4_loss_6: 0.0159 - dense_4_loss_7: 0.0937 - dense_4_loss_8: 0.0015 - dense_4_loss_9: 0.0235 - dense_4_loss_10: 0.0482 - dense_4_acc_1: 0.9973 - dense_4_acc_2: 0.9973 - dense_4_acc_3: 0.9892 - dense_4_acc_4: 0.9945 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9955 - dense_4_acc_7: 0.9779 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9988 - dense_4_acc_10: 0.9944\n",
      "Epoch 12/20\n",
      " - 21s - loss: 0.2961 - dense_4_loss_1: 0.0089 - dense_4_loss_2: 0.0070 - dense_4_loss_3: 0.0492 - dense_4_loss_4: 0.0530 - dense_4_loss_5: 0.0011 - dense_4_loss_6: 0.0157 - dense_4_loss_7: 0.0918 - dense_4_loss_8: 0.0014 - dense_4_loss_9: 0.0216 - dense_4_loss_10: 0.0464 - dense_4_acc_1: 0.9975 - dense_4_acc_2: 0.9973 - dense_4_acc_3: 0.9893 - dense_4_acc_4: 0.9944 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9953 - dense_4_acc_7: 0.9784 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9989 - dense_4_acc_10: 0.9945\n",
      "Epoch 13/20\n",
      " - 22s - loss: 0.2869 - dense_4_loss_1: 0.0086 - dense_4_loss_2: 0.0068 - dense_4_loss_3: 0.0477 - dense_4_loss_4: 0.0501 - dense_4_loss_5: 0.0011 - dense_4_loss_6: 0.0151 - dense_4_loss_7: 0.0895 - dense_4_loss_8: 0.0015 - dense_4_loss_9: 0.0212 - dense_4_loss_10: 0.0453 - dense_4_acc_1: 0.9979 - dense_4_acc_2: 0.9977 - dense_4_acc_3: 0.9900 - dense_4_acc_4: 0.9951 - dense_4_acc_5: 0.9999 - dense_4_acc_6: 0.9949 - dense_4_acc_7: 0.9790 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9992 - dense_4_acc_10: 0.9952\n",
      "Epoch 14/20\n",
      " - 22s - loss: 0.2764 - dense_4_loss_1: 0.0084 - dense_4_loss_2: 0.0065 - dense_4_loss_3: 0.0455 - dense_4_loss_4: 0.0480 - dense_4_loss_5: 9.6726e-04 - dense_4_loss_6: 0.0149 - dense_4_loss_7: 0.0871 - dense_4_loss_8: 0.0014 - dense_4_loss_9: 0.0202 - dense_4_loss_10: 0.0435 - dense_4_acc_1: 0.9974 - dense_4_acc_2: 0.9978 - dense_4_acc_3: 0.9904 - dense_4_acc_4: 0.9958 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9962 - dense_4_acc_7: 0.9809 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9995 - dense_4_acc_10: 0.9956\n",
      "Epoch 15/20\n",
      " - 22s - loss: 0.2685 - dense_4_loss_1: 0.0081 - dense_4_loss_2: 0.0061 - dense_4_loss_3: 0.0436 - dense_4_loss_4: 0.0459 - dense_4_loss_5: 0.0010 - dense_4_loss_6: 0.0145 - dense_4_loss_7: 0.0854 - dense_4_loss_8: 0.0014 - dense_4_loss_9: 0.0197 - dense_4_loss_10: 0.0428 - dense_4_acc_1: 0.9979 - dense_4_acc_2: 0.9981 - dense_4_acc_3: 0.9910 - dense_4_acc_4: 0.9957 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9960 - dense_4_acc_7: 0.9801 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9993 - dense_4_acc_10: 0.9957\n",
      "Epoch 16/20\n",
      " - 22s - loss: 0.2617 - dense_4_loss_1: 0.0078 - dense_4_loss_2: 0.0058 - dense_4_loss_3: 0.0422 - dense_4_loss_4: 0.0444 - dense_4_loss_5: 9.5585e-04 - dense_4_loss_6: 0.0145 - dense_4_loss_7: 0.0840 - dense_4_loss_8: 0.0013 - dense_4_loss_9: 0.0192 - dense_4_loss_10: 0.0416 - dense_4_acc_1: 0.9979 - dense_4_acc_2: 0.9982 - dense_4_acc_3: 0.9918 - dense_4_acc_4: 0.9959 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9959 - dense_4_acc_7: 0.9808 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9995 - dense_4_acc_10: 0.9951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      " - 21s - loss: 0.2521 - dense_4_loss_1: 0.0075 - dense_4_loss_2: 0.0055 - dense_4_loss_3: 0.0403 - dense_4_loss_4: 0.0425 - dense_4_loss_5: 9.2088e-04 - dense_4_loss_6: 0.0137 - dense_4_loss_7: 0.0822 - dense_4_loss_8: 0.0013 - dense_4_loss_9: 0.0183 - dense_4_loss_10: 0.0399 - dense_4_acc_1: 0.9980 - dense_4_acc_2: 0.9982 - dense_4_acc_3: 0.9919 - dense_4_acc_4: 0.9963 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9961 - dense_4_acc_7: 0.9812 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9994 - dense_4_acc_10: 0.9961\n",
      "Epoch 18/20\n",
      " - 22s - loss: 0.2473 - dense_4_loss_1: 0.0073 - dense_4_loss_2: 0.0054 - dense_4_loss_3: 0.0391 - dense_4_loss_4: 0.0411 - dense_4_loss_5: 9.5703e-04 - dense_4_loss_6: 0.0136 - dense_4_loss_7: 0.0809 - dense_4_loss_8: 0.0012 - dense_4_loss_9: 0.0181 - dense_4_loss_10: 0.0395 - dense_4_acc_1: 0.9981 - dense_4_acc_2: 0.9981 - dense_4_acc_3: 0.9918 - dense_4_acc_4: 0.9965 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9960 - dense_4_acc_7: 0.9814 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9995 - dense_4_acc_10: 0.9955\n",
      "Epoch 19/20\n",
      " - 22s - loss: 0.2388 - dense_4_loss_1: 0.0071 - dense_4_loss_2: 0.0052 - dense_4_loss_3: 0.0376 - dense_4_loss_4: 0.0393 - dense_4_loss_5: 9.0878e-04 - dense_4_loss_6: 0.0130 - dense_4_loss_7: 0.0789 - dense_4_loss_8: 0.0012 - dense_4_loss_9: 0.0174 - dense_4_loss_10: 0.0382 - dense_4_acc_1: 0.9982 - dense_4_acc_2: 0.9985 - dense_4_acc_3: 0.9929 - dense_4_acc_4: 0.9968 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9961 - dense_4_acc_7: 0.9820 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9995 - dense_4_acc_10: 0.9957\n",
      "Epoch 20/20\n",
      " - 22s - loss: 0.2320 - dense_4_loss_1: 0.0068 - dense_4_loss_2: 0.0048 - dense_4_loss_3: 0.0363 - dense_4_loss_4: 0.0380 - dense_4_loss_5: 8.7969e-04 - dense_4_loss_6: 0.0130 - dense_4_loss_7: 0.0770 - dense_4_loss_8: 0.0012 - dense_4_loss_9: 0.0171 - dense_4_loss_10: 0.0370 - dense_4_acc_1: 0.9985 - dense_4_acc_2: 0.9986 - dense_4_acc_3: 0.9927 - dense_4_acc_4: 0.9968 - dense_4_acc_5: 1.0000 - dense_4_acc_6: 0.9965 - dense_4_acc_7: 0.9836 - dense_4_acc_8: 1.0000 - dense_4_acc_9: 0.9993 - dense_4_acc_10: 0.9962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a585aa9ac8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_ohe, ini_acti_post, ini_mem_post], list(Y_ohe.swapaxes(0,1)), epochs=20, batch_size=100, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "model.save_weights(r'models/weights_55.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Once the model has been trained it is time to check its performance on new data and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  may 13 2010\n",
      "Output:  2010-05-13\n",
      "Target Output:  2010-05-13\n",
      "\n",
      "Input:  2 november 2007\n",
      "Output:  2007-11-02\n",
      "Target Output:  2007-11-02\n",
      "\n",
      "Input:  12 september 2016\n",
      "Output:  2016-09-12\n",
      "Target Output:  2016-09-12\n",
      "\n",
      "Input:  9 12 86\n",
      "Output:  1986-12-09\n",
      "Target Output:  1986-12-09\n",
      "\n",
      "Input:  f1984llf1984llf1984ll\n",
      "Output:  1883333333\n",
      "Target Output:  1984-10-10\n",
      "\n",
      "Input:  8 apr 1999\n",
      "Output:  1999-04-08\n",
      "Target Output:  1999-04-08\n",
      "\n",
      "Input:  5 july 1992\n",
      "Output:  1992-07-05\n",
      "Target Output:  1992-07-05\n",
      "\n",
      "Input:  04.07.05\n",
      "Output:  2005-07-04\n",
      "Target Output:  2005-07-04\n",
      "\n",
      "Input:  14 nov 2010\n",
      "Output:  2010-11-14\n",
      "Target Output:  2010-11-14\n",
      "\n",
      "Input:  10 september 1996\n",
      "Output:  1996-09-10\n",
      "Target Output:  1996-09-10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate new date samples\n",
    "dates, _, _ = create_dataset(10)\n",
    "vocab_len = len(human_vocab)\n",
    "\n",
    "for date, machine in dates:\n",
    "    # truncate the length of date if it exceeds Tx\n",
    "    if len(date) > Tx:\n",
    "        date = date[:Tx]\n",
    "        \n",
    "    # some preprocessing\n",
    "    date = date.lower().replace(',', '')        \n",
    "    source = np.zeros((1, Tx, vocab_len))\n",
    "    \n",
    "    # make OHE of date\n",
    "    for t, char in enumerate(date):\n",
    "        source[0, t, human_char_idx[char]] = 1\n",
    "    \n",
    "    prediction = model.predict([source, ini_acti_post, ini_mem_post])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [machine_idx_char[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"Input: \", date)\n",
    "    print(\"Output: \", ''.join(output))\n",
    "    print('Target Output: ', machine)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Credits:\n",
    "This project is based on the assignment from Sequence Models Specialization by Deeplearning.ai on Coursera. <br>https://www.coursera.org/learn/nlp-sequence-models/home/welcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
